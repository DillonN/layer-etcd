#!/bin/bash

set -ex

# This is an encapsulation of the "Restoring a Backup" section of the Etcd
# administration guide located here: https://coreos.com/etcd/docs/latest/admin_guide.html
#
# Note: That restoration of a clusters data *must* be performed on a single etcd
# cluster node, in a new state. Restoration of existing cluster state is not
# supported at this time.

# this annoys my OCD
DATESTAMP=$(date +%Y-%m-%d-%H.%M.%S)

ARCHIVE=etcd-data-$DATESTAMP.tar.gz
ETCD_DATA_DIR=/var/snap/etcd/current/
ETCD_PORT=$(config-get management_port)
PRIVATE_ADDRESS=$(unit-get private-address)
SKIP_BACKUP=$(action-get skip-backup)
SNAPSHOT_ARCHIVE=$(resource-get snapshot)
TARGET_PATH=$(action-get target)

# Having a snapshot loaded in the resource stream is mandatory for a Restore
# to function.
if [ -z "${SNAPSHOT_ARCHIVE}" ]
then
  action-set result.failed="Missing snapshot archive. See: README.md"
  exit 0
fi

# Check for the existence of an existing cluster data-store. This will be
# true 99% of the time, as even new clusters installed from apt seem to initialize
# some form of data-store

if [ -d $ETCD_DATA_DIR ]
then
  cd $ETCD_DATA_DIR

  # might cause the action to fail, but not sure if its better to get the output
  # or to set a message and irritate the user, or just blindly delete their data.
  if [ "${SKIP_BACKUP}" != 'true' ]
  then
    juju-log "Backing up existing data found in $ETCD_DATA_DIR"
    tar cvf $TARGET_PATH/$ARCHIVE default
    action-set backup.path="$TARGET_PATH/$ARCHIVE"
    action-set backup.sha256sum=$(sha256sum $TARGET_PATH/$ARCHIVE | cut -d' ' -f1)
  fi
  rm -rf default
  mkdir default
  cd $CHARM_DIR
fi

juju-log "Stopping Etcd to perform the restore."
systemctl stop snap.etcd.etcd

# Give systemd and etcd a couple seconds to catch up. systemctl is blocking
# so this is likely uneccesary other than ensuring we're not racing.
sleep 3

# Unpack the resource
tar xvfz $SNAPSHOT_ARCHIVE -C $ETCD_DATA_DIR/default
chown -R etcd:etcd $ETCD_DATA_DIR

# Fork and start the etcd node in the background to drop any peer/client information
etcd -data-dir=$ETCD_DATA_DIR/default -wal-dir=$ETCD_DATA_DIR/default/member/wal -force-new-cluster &

# Get persinickety about the error state here, as this will fail if we race with
# systemd and etcd.
set +x
until etcdctl member list | grep "http://localhost"; do
  sleep 1
done
set -x

# Reconfigure the data-store peer-url on the fly. This will fail if there are
# more than a single unit. #TODO Make this more robust. This was necessary when
# scaling the unit. The new-init seems to be over-riding the advertise flags set
# in the defaults file.
ETCD_CLUSTER_UNIT_ID=$(etcdctl member list | cut -d ':' -f1)
etcdctl member update $ETCD_CLUSTER_UNIT_ID https://$PRIVATE_ADDRESS:$ETCD_PORT

until etcdctl member list | grep "https://$PRIVATE_ADDRESS:$ETCD_PORT"; do
  juju-log "Waiting on etcd peer url configuration to propigate"
  # give this a bit of a longer timeout to ensure the data has settled once we
  # break
  sleep 5
done
# terminate the instance and resume normal operation
pkill etcd

systemctl restart etcd
